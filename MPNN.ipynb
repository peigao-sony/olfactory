{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a9f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import ChemicalFeatures\n",
    "from rdkit import RDConfig\n",
    "import networkx as nx\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.variable import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pickle\n",
    "import copy\n",
    "import datetime\n",
    "import jsonpickle\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f42f4b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.SecondOrder import szymanski_ts_eq_fold\n",
    "from functions.GraphReader import qm9_edges,qm9_nodes\n",
    "from functions.GraphReader import get_values,get_graph_stats\n",
    "\n",
    "from functions.Utlis import AverageMeter\n",
    "\n",
    "from models.MPNN import MPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba95f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd71162",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_label = pd.read_csv('dataset.csv',index_col=0)\n",
    "#label = label.iloc[:12,:]\n",
    "dat_smiles = pd.read_csv('smiles.csv', index_col=0)\n",
    "dict_smile = dat_smiles.to_dict()['SMILES']\n",
    "# np.save('dict_smile.npy', dict_smile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124de90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_labels = dat_label.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47edf5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558155b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "lst_imbalance_ratio = []\n",
    "for i in dat_label.values:\n",
    "    class_weights=compute_class_weight('balanced',classes=np.unique(i), y=i)\n",
    "    ratio = class_weights[1]\n",
    "    lst_imbalance_ratio.append(ratio)\n",
    "\n",
    "import math\n",
    "weights = []\n",
    "for i in range(len(lst_imbalance_ratio)):\n",
    "    weights.append(math.log(1+lst_imbalance_ratio[i]))\n",
    "weights = torch.tensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77420b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a4f0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### split n-folds\n",
    "#from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy.sparse import lil_matrix\n",
    "lil_matrix(dat_labels.values)\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "folds =szymanski_ts_eq_fold(n_splits, dat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fabc127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab91d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--prefetch'], dest='prefetch', nargs=None, const=None, default=4, type=<class 'int'>, choices=None, help='Pre-fetching threads.', metavar=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parser check\n",
    "def restricted_float(x, inter):\n",
    "    x = float(x)\n",
    "    if x < inter[0] or x > inter[1]:\n",
    "        raise argparse.ArgumentTypeError(\"%r not in range [1e-5, 1e-4]\"%(x,))\n",
    "    return x\n",
    "\n",
    "# Argument parser\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Neural message passing')\n",
    "\n",
    "#parser.add_argument('--dataset', default='qm9', help='QM9')\n",
    "#parser.add_argument('--datasetPath', default='/home/taobai/Documents/Model/MPNN/mpnn-data/qm9/dsgdb9nsd/', help='dataset path')\n",
    "#parser.add_argument('--logPath', default='./log/qm9/mpnn/', help='log path')\n",
    "#parser.add_argument('--plotLr', default=False, help='allow plotting the data')\n",
    "#parser.add_argument('--plotPath', default='./plot/qm9/mpnn/', help='plot path')\n",
    "#parser.add_argument('--resume', default='./checkpoint/qm9/mpnn/',\n",
    "#                    help='path to latest checkpoint')\n",
    "\n",
    "# Optimization Options\n",
    "parser.add_argument('--batch-size', type=int, default=10, metavar='N',\n",
    "                    help='Input batch size for training (default: 20)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Enables CUDA training')\n",
    "parser.add_argument('--epochs', type=int, default=300, metavar='N',\n",
    "                    help='Number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=lambda x: restricted_float(x, [1e-5, 1e-2]), default=1e-4, metavar='LR',\n",
    "                    help='Initial learning rate [1e-5, 5e-4] (default: 1e-4)')\n",
    "parser.add_argument('--lr-decay', type=lambda x: restricted_float(x, [.01, 1]), default=0.5, metavar='LR-DECAY',\n",
    "                    help='Learning rate decay factor [.01, 1] (default: 0.6)')\n",
    "parser.add_argument('--schedule', type=list, default=[0.1, 0.9], metavar='S',\n",
    "                    help='Percentage of epochs to start the learning rate decay [0, 1] (default: [0.1, 0.9])')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "# i/o\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                    help='How many batches to wait before logging training status')\n",
    "# Accelerating\n",
    "parser.add_argument('--prefetch', type=int, default=4, help='Pre-fetching threads.')\n",
    "\n",
    "# Model modification\n",
    "# parser.add_argument('--model', type=str,help='MPNN model name [MPNN, MPNNv2, MPNNv3]',default='MPNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff5b30f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=10, no_cuda=False, epochs=300, lr=0.0001, lr_decay=0.5, schedule=[0.1, 0.9], momentum=0.9, log_interval=100, prefetch=4)\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b57531a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c234a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_er1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f98f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e882a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyz_graph_reader(CID):\n",
    "    dict_smile = np.load('dict_smile.npy',allow_pickle=True).item()\n",
    "    smiles = dict_smile[CID]\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    m = Chem.AddHs(m)\n",
    "   \n",
    "    g = nx.Graph()\n",
    "    # Create nodes\n",
    "    for i in range(0, m.GetNumAtoms()):\n",
    "        atom_i = m.GetAtomWithIdx(i)\n",
    "\n",
    "        g.add_node(i, a_type=atom_i.GetSymbol(), a_num=atom_i.GetAtomicNum(), acceptor=0, donor=0,\n",
    "                   aromatic=atom_i.GetIsAromatic(), hybridization=atom_i.GetHybridization(),\n",
    "                   num_h=atom_i.GetTotalNumHs())\n",
    "\n",
    "\n",
    "    # Read Edges\n",
    "    for i in range(0, m.GetNumAtoms()):\n",
    "        for j in range(0, m.GetNumAtoms()):\n",
    "            e_ij = m.GetBondBetweenAtoms(i, j)\n",
    "            if e_ij is not None:\n",
    "                g.add_edge(i, j, b_type=e_ij.GetBondType())\n",
    "            else:\n",
    "                # Unbonded\n",
    "                g.add_edge(i, j, b_type=None)\n",
    "\n",
    "    l = list(dat_label[CID].values)           \n",
    "                \n",
    "    return g , l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea03898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qm9():\n",
    "#class Qm9(data.Dataset):\n",
    "    # Constructor\n",
    "    def __init__(self, idx, vertex_transform=qm9_nodes, edge_transform=qm9_edges,\n",
    "                 target_transform=None, e_representation='raw_distance'):\n",
    "        self.idx = idx\n",
    "        self.vertex_transform = vertex_transform\n",
    "        self.edge_transform = edge_transform\n",
    "        self.target_transform = target_transform\n",
    "        self.e_representation = e_representation\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        g, target = xyz_graph_reader(self.idx[index])\n",
    "        if self.vertex_transform is not None:\n",
    "            h = self.vertex_transform(g)\n",
    "\n",
    "        if self.edge_transform is not None:\n",
    "            g, e = self.edge_transform(g)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        #g：adjacent matrix\n",
    "        #h：node properties（list of list）\n",
    "        #e：diction，key:edge，value:properties\n",
    "        return (g, h, e), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def set_target_transform(self, target_transform):\n",
    "        self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2ec87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcd8dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be9d6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_g(batch):\n",
    "\n",
    "    batch_sizes = np.max(np.array([[len(input_b[1]), len(input_b[1][0]), len(input_b[2]),\n",
    "                                len(list(input_b[2].values())[0])]\n",
    "                                if input_b[2] else\n",
    "                                [len(input_b[1]), len(input_b[1][0]), 0,0]\n",
    "                                for (input_b, target_b) in batch]), axis=0)\n",
    "\n",
    "    g = np.zeros((len(batch), batch_sizes[0], batch_sizes[0]))\n",
    "    h = np.zeros((len(batch), batch_sizes[0], batch_sizes[1]))\n",
    "    e = np.zeros((len(batch), batch_sizes[0], batch_sizes[0], batch_sizes[3]))\n",
    "\n",
    "    target = np.zeros((len(batch), len(batch[0][1])))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "\n",
    "        num_nodes = len(batch[i][0][1])\n",
    "\n",
    "        # Adjacency matrix\n",
    "        g[i, 0:num_nodes, 0:num_nodes] = batch[i][0][0]\n",
    "\n",
    "        # Node features\n",
    "        h[i, 0:num_nodes, :] = batch[i][0][1]\n",
    "\n",
    "        # Edges\n",
    "        for edge in batch[i][0][2].keys():\n",
    "            e[i, edge[0], edge[1], :] = batch[i][0][2][edge]\n",
    "            e[i, edge[1], edge[0], :] = batch[i][0][2][edge]\n",
    "\n",
    "        # Target\n",
    "        target[i, :] = batch[i][1]\n",
    "\n",
    "    g = torch.FloatTensor(g)\n",
    "    h = torch.FloatTensor(h)\n",
    "    e = torch.FloatTensor(e)\n",
    "    target = torch.FloatTensor(target)\n",
    "\n",
    "    return g, h, e, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155c6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db01307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, logger=None):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "    \n",
    "    #list_loss = []\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (g, h, e, target) in enumerate(train_loader):\n",
    "\n",
    "        # Prepare input data\n",
    "        if args.cuda:\n",
    "            g, h, e, target = g.cuda(), h.cuda(), e.cuda(), target.cuda()\n",
    "        g, h, e, target = Variable(g), Variable(h), Variable(e), Variable(target)\n",
    "\n",
    "        # Measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute output\n",
    "        output = model(g, h, e)\n",
    "        train_loss = criterion(output, target)\n",
    "        #list_loss.append(train_loss.item())\n",
    "        # Logs\n",
    "        losses.update(train_loss.item(), g.size(0))\n",
    "        accuracy.update(evaluation(output, target).item(), g.size(0))\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.log_interval == 0 and i > 0:\n",
    "\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Accuracy {err.val:.4f} ({err.avg:.4f})'\n",
    "                  .format(epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                          data_time=data_time, loss=losses, err=accuracy))\n",
    "\n",
    "    print('Epoch: [{0}] Accuracy {err.avg:.3f}; Average Loss {loss.avg:.3f}; Avg Time x Batch {b_time.avg:.3f}'\n",
    "          .format(epoch, err=accuracy, loss=losses, b_time=batch_time))\n",
    "    \n",
    "    #return list_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f94cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, evaluation, logger=None):\n",
    "#     batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (g, h, e, target) in enumerate(val_loader):\n",
    "\n",
    "        # Prepare input data\n",
    "        if args.cuda:\n",
    "            g, h, e, target = g.cuda(), h.cuda(), e.cuda(), target.cuda()\n",
    "        g, h, e, target = Variable(g), Variable(h), Variable(e), Variable(target)\n",
    "\n",
    "        # Compute output\n",
    "        output = model(g, h, e)\n",
    "\n",
    "        # Logs\n",
    "        losses.update(criterion(output, target).item(), g.size(0))\n",
    "        accuracy.update(evaluation(output, target).item(), g.size(0))\n",
    "\n",
    "#         # measure elapsed time\n",
    "#         batch_time.update(time.time() - end)\n",
    "#         end = time.time()\n",
    "\n",
    "    print(' * Average accuracy {err.avg:.3f}; '\n",
    "          .format(err=accuracy))\n",
    "\n",
    "\n",
    "    return accuracy.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873cd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c8dcdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cl = []\n",
    "result_auc = []\n",
    "initial_lr = args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0177c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_folds in range(5):\n",
    "    args.lr = initial_lr\n",
    "    idxes = []\n",
    "    for i in range(5):\n",
    "        idxes.append(i)\n",
    "\n",
    "    test_folds = int(n_folds)\n",
    "    train_folds = idxes[:]\n",
    "    train_folds.remove(n_folds)\n",
    "\n",
    "    test_ids = [dat_label.columns[i] for i in folds[test_folds]]\n",
    "    train_ids =  [dat_label.columns[i] for j in train_folds for i in folds[j]]\n",
    "\n",
    "    data_train = Qm9(train_ids, edge_transform=qm9_edges, e_representation='raw_distance')\n",
    "    data_test = Qm9(test_ids, edge_transform=qm9_edges, e_representation='raw_distance')\n",
    "\n",
    "    # Select one graph\n",
    "    g_tuple, l = data_train[0]\n",
    "    g, h_t, e = g_tuple\n",
    "\n",
    "    stat_dict = get_graph_stats(data_train[0])\n",
    "\n",
    "    # set_target_transform was defined in Class Qm9\n",
    "    data_train.set_target_transform(lambda x:x)\n",
    "    data_test.set_target_transform(lambda x: x)\n",
    "    \n",
    "    \n",
    "    # collate_g from utils\n",
    "    train_loader = torch.utils.data.DataLoader(data_train,\n",
    "                                           batch_size=args.batch_size,shuffle=True, collate_fn=collate_g,\n",
    "                                           num_workers=args.prefetch, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(data_test,\n",
    "                                          batch_size=args.batch_size, collate_fn=collate_g,\n",
    "                                          num_workers=args.prefetch, pin_memory=True)\n",
    "\n",
    "    #点的特征维度，边的特征维度\n",
    "    in_n = [len(h_t[0]), len(list(e.values())[0])] \n",
    "    #hidden state/embedding维度\n",
    "    hidden_state_size = 20\n",
    "    #邻居消息m_i维度（聚合后的维度）后面都用d_v表示\n",
    "    message_size = 20\n",
    "    #GNN层数\n",
    "    n_layers = 3\n",
    "    #labels数量\n",
    "    l_target = len(l)\n",
    "    #回归任务\n",
    "    type ='classification'\n",
    "    #type ='regression'\n",
    "\n",
    "    #定义mpnn模型\n",
    "    model = MPNN(in_n, hidden_state_size, message_size, n_layers, l_target, type=type)\n",
    "\n",
    "    del in_n, hidden_state_size, message_size, n_layers, l_target, type\n",
    "\n",
    "    #print('Optimizer')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "    #回归任务使用MSE 1ose\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=weights,reduction='mean')\n",
    "\n",
    "    #评估指标，|a-b|/|b|\n",
    "    #evaluation = lambda output, target: torch.mean(torch.abs(output - target) / torch.abs(target))\n",
    "    evaluation = lambda output, target: torch.eq(torch.round(output),target).float().mean()\n",
    "\n",
    "\n",
    "    lr_step = (args.lr-args.lr*args.lr_decay)/(args.epochs*args.schedule[1] - args.epochs*args.schedule[0])\n",
    "\n",
    "    print('Check cuda')\n",
    "    if args.cuda:\n",
    "        print('\\t* Cuda')\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.cuda()\n",
    "\n",
    "\n",
    "    # Epoch for loop\n",
    "    for epoch in range(0, args.epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if epoch > args.epochs * args.schedule[0] and epoch < args.epochs * args.schedule[1]:\n",
    "            args.lr -= lr_step\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = args.lr\n",
    "\n",
    "        # train for one epoch\n",
    "        # lst_loss = train(train_loader, model, criterion, optimizer, epoch)\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "        # evaluate on test set\n",
    "        #er1 = validate(valid_loader, model, criterion, evaluation)\n",
    "\n",
    "        #is_best = er1 > best_er1\n",
    "        #best_er1 = min(er1, best_er1)\n",
    "\n",
    "        \n",
    "    # For testing\n",
    "    report = []\n",
    "    all_pred = []\n",
    "    all_target =[]\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "\n",
    "    if args.cuda:\n",
    "        print('\\t* Cuda')\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    for i, (g, h, e, target) in enumerate(test_loader):\n",
    "        torch.cuda.empty_cache()\n",
    "        # Prepare input data\n",
    "        if args.cuda:\n",
    "            g, h, e, target = g.cuda(), h.cuda(), e.cuda(), target.cuda()\n",
    "        g, h, e, target = Variable(g), Variable(h), Variable(e), Variable(target)\n",
    "\n",
    "        # Compute output\n",
    "        output =model(g, h, e)\n",
    "        preds = torch.round(output)\n",
    "\n",
    "        all_pred.append(preds.cpu().detach().numpy())\n",
    "        all_target.append(target.cpu().detach().numpy())\n",
    "\n",
    "        \n",
    "        \n",
    "    all_pred = np.vstack(all_pred)\n",
    "    all_target = np.vstack(all_target)\n",
    "    \n",
    "    lst_auc = []\n",
    "    for i in range(91):\n",
    "        lst_auc.append(roc_auc_score(np.transpose(all_target)[i], np.transpose(all_pred)[i]))\n",
    "    lst_auc.append(roc_auc_score(all_target, all_pred))\n",
    "\n",
    "    cl = classification_report(all_target, all_pred,output_dict=True)\n",
    "    report = pd.DataFrame.from_dict(cl, orient='index')\n",
    "    \n",
    "    result_cl.append(report)\n",
    "    result_auc.append(lst_auc)\n",
    "    \n",
    "    del model,optimizer,g, h, e, target, output, preds, criterion\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cl = result_cl[0]\n",
    "for i in range(1,5):\n",
    "    results_cl = results_cl + result_cl[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aabfe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_auc = (np.sum([i for i in result_auc], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afb5d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cl.to_csv('12-15-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4bb905",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results_auc:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
